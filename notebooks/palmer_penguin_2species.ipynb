{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to ML for EEB, session 2\n",
    "\n",
    "Hello! This notebook demonstrates how to make a classifier to distinguish between penguin species.\n",
    "\n",
    "This notebook will be in Python, which some of you may be unfamiliar with. In my opinion, the limiting factor in data analysis and ML is asking the right questions: ***what*** questions should you ask about your data to better understand it? \n",
    "\n",
    "***How*** to do it is another issue, but going in a good direction in the first place is the most important (so having a good advisor is nice :) ). Once you have a good direction, nowadays we can just use AI to code with us so that we don't need to remember a bunch of syntax, but we do need to double check everything.\n",
    "\n",
    "Once you've defined ***what*** to do, most coding lanuages these days allow you to do everything the other language can. Popularity can make a language nicer to know, since you have more code to use from other people.\n",
    "\n",
    "I'll try to explain how the code works, and if my explanations don't make sense, I find chatGPT gives very good explanations of code snippets (regardless of the language).\n",
    "\n",
    "This notebook will illustrate a typical machine learning (ML) workflow, where we will use 2 different base ML models: logistic regression and decision trees. We will unfortunately not use neural networks, because the data we'll analyze is too simple, and simple approaches perform quite well already!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's common practice, but not required, to import all packages you'll use at the top of a file\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, to_rgb\n",
    "#!pip install palmerpenguins\n",
    "from palmerpenguins import load_penguins\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a few things\n",
    "\n",
    "Colors to use for plotting, 1 for each species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors selected from https://coolors.co/palettes/trending\n",
    "pal = [\"#219ebc\", \"#fb8500\", \"#023047\"] # 3 for the 3 species\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select species to analyze.\n",
    "\n",
    "Today we'll only distinguish between two species to keep things simple, although I originally wrote this workshop to distinguish between all 3: Adelie, Gentoo, and Chinstrapp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_to_analyze = ['Adelie',  'Gentoo']\n",
    "\n",
    "# a sanity check to prevent everything downstream from breaking :)\n",
    "assert len(species_to_analyze) > 1, \"You need to select more than one species to analyze!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select difficulty\n",
    "\n",
    "Let's start out easy, but set this variable to True later and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the penguin data\n",
    "See [here](https://allisonhorst.github.io/palmerpenguins/) for a description of the data and the features we'll use. \n",
    "\n",
    "Machine learning (ML) involves building a model to predict a response using other features that we've measured. \n",
    "\n",
    "These features must have some *informative* relationship with the response: as a feature changes values, so does the response. \n",
    "\n",
    "\n",
    "Since today's data are relatively simple, we can visualize the relationship between the features and the response with plots! Intuitively, if we're trying to distinguish between species, they should have systematic differences in 1 or more features!\n",
    "\n",
    "ADVANCED NOTE: in practice, we may have measured *many* features, only some are informative, and we aren't sure which ones. Many ML algorithms can perform *feature selection* in which they find the informative features and only use those. Today, we have preselected informative features.\n",
    "\n",
    "## Load dataset into a \"pandas DataFrame\"\n",
    "\n",
    "A 'DataFrame' is a simple way to represent data with rows and columns. It's similar to what you'd see in R, but the commands you use to manipulate/access it are different because it's Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data we downloaded with the 'palmerpenguins' package\n",
    "penguins = load_penguins()\n",
    "\n",
    "# INSPECT DataFrame\n",
    "penguins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data\n",
    "\n",
    "All of this is done in one code cell, where the functions are all run back-to-back. In practice, this usually involves a trial-and-error process where we realize we need to process the data in different ways, but I put all these functions in one code cell to make this clean and modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD COLUMN that assigns each species a color, useful to make colors consistent across plots\n",
    "# this is NOT a necessary step, I just wanted things to looks pretty and consistent\n",
    "penguins[\"color\"] = penguins[\"species\"].map({\"Adelie\": pal[0], \n",
    "                                             \"Gentoo\": pal[1],\n",
    "                                             \"Chinstrap\": pal[2]})\n",
    "\n",
    "# EXCLUDE ROWS with missing data \n",
    "# we only exclude rows where there is missing data for any of the variables listed in the subset argument, which we give a list of names\n",
    "penguins = penguins.dropna(subset = [\"bill_length_mm\", \n",
    "                                     \"bill_depth_mm\", \n",
    "                                     \"species\", \n",
    "                                     \"flipper_length_mm\", \n",
    "                                     \"body_mass_g\"])\n",
    "\n",
    "# SELECT ROWS containing the species of interest, specified above\n",
    "penguins = penguins.loc[penguins['species'].isin(species_to_analyze)]\n",
    "\n",
    "# RESET INDEX, since some rows may have been eliminated from selecting species\n",
    "penguins.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# fraudulently manipulate data to demonstrate ML with messier signals\n",
    "if hard_mode == True:\n",
    "    penguins['bill_length_mm'] = np.where(penguins['species'] == 'Adelie', \n",
    "                                          penguins['bill_length_mm']+4, \n",
    "                                          penguins['bill_length_mm'])\n",
    "    penguins['bill_depth_mm'] = np.where(penguins['species'] == 'Adelie', \n",
    "                                          penguins['bill_depth_mm']-3, \n",
    "                                          penguins['bill_depth_mm'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, it looked like these species are distributed among several islands and collected across several years.\n",
    "\n",
    "What do these categorical variables look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins['island'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot features to assess relationship with response (species)\n",
    "\n",
    "Here, we will use the seaborn 'pairplot' function, which produces a lot of info, a martix of plots. At first I made some plots using some data wrangling along with some other plotting functions, but my AI bot (github copilot) proposed this one line solution that made code much simpler.\n",
    "\n",
    "We will tell pairplot to color the data by species using the 'hue' argument, and we will select only some of the columns form the DataFrame (ignoring the 'year')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.pairplot(penguins[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'species']], \n",
    "             hue=\"species\", palette=pal[0:2], diag_kind=\"kde\", diag_kws=dict( alpha=0.5, linewidth=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure compares features in a pairwise fashion. \n",
    "\n",
    "The diagonals are comparing a feature with itself, so it just plots a histogram of the feature.\n",
    "\n",
    "In the off-diagonals, it's comparing features in a pairwise fashion using a scatterplot.\n",
    "\n",
    "One interesting thing that immediately emerges from this is that the species seem to differ according to all these features, but there's always some overlap (digonal plots).\n",
    "\n",
    "However, when you look at any two of these features simultaneously (off-diagonal plots), the two species are **highly** separable, so using multiple features can be powerful. ML algorithms will see this huge difference in two dimensions, and make a 'decision boundary' right between the cleanly separated species. If data are on one side of the decision boundary, they get classified as species 1, if they're on the other side, species 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thought: body mass can vary depending on sex. Are sex ratios of these samples very different between species? Specifically, is body mass generally greater in one species because we've systematically sampled more males from that species?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the 'species' and 'sex' columns, group the values by 'species' species, and count the sex\n",
    "penguins[['species', 'sex']].groupby('species').value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed with the bill features.\n",
    "\n",
    "While we've manually gone through our dataset to get an intuition of the relationship between the features and response, this can be tedious if we've measured thousands of features. In these cases, there are approaches to automate this and select the best features.\n",
    "\n",
    "Let's store our data in more informatively-named tables: `features` as `X`, and `response` as `Y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to add more features!\n",
    "X = penguins[[\"bill_length_mm\", \"bill_depth_mm\"]]\n",
    "Y = penguins[\"species\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into 2 parts: training and testing\n",
    "Again, we could do this ourselves by randomly sampling penguins from our data, but scikit-learn has a `train_test_split` function to do this for us. Specify what fraction of the data you want to use as the test set using the `test_size` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also set the random_state to ensure we all get the same results!\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=11, \n",
    "                                                    shuffle=True)\n",
    "\n",
    "# plot the 'shape' or 'dimensions' of the data (or number of rows and columns if matrix is 2D)\n",
    "print(\"X_normalized dimensions \", X.shape)\n",
    "print(\"X_train dimensions \", X_train.shape)\n",
    "print(\"X_test dimensions \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just quick peek at the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature rescaling\n",
    "\n",
    "It's best practice to **rescale** our features so that they all have similar values and ranges. Not doing so can lead to models with bad predictive performance!\n",
    "\n",
    " Here we will use a function from scikit learn that performs 'z-score normalization': for each feature, we will take it's value, subtract the mean, and divide by the standard deviation. This makes all features have a mean of 0 with standard deviation of 1.\n",
    "\n",
    " $$ z = {x - \\mu_x \\over \\sigma_x} $$\n",
    "\n",
    "We could do this normalization ourselves, but the scikit-learn package has several functions to normalize your data, so let's use these. I always try to write as little new code as possible, to keep my code as simple, clean, and bug-free as possible.\n",
    "\n",
    "\n",
    "Lastly, let's make a plot of the features before and after. Sanity checks are **always** a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the scaler, and have it output the data as a pandas DataFrame, for ease of use\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "# Fit the scaler to the training data, i.e. compute the mean and std for the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform the training and test data, here the test data was transformed using prior information from the training data (mean and variance)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# plot the features before and after rescaling\n",
    "\n",
    "#specify plot layout and size\n",
    "fig, axs = plt.subplots(1, 3, figsize=(13, 4))\n",
    "\n",
    "# plot histogram of original features\n",
    "sns.histplot(X, bins=20, ax=axs[0])\n",
    "axs[0].set_title(\"Before normalization\")\n",
    "\n",
    "# plot histogram of TRANSFORMED features\n",
    "sns.histplot(X_train, bins=20, ax=axs[1])\n",
    "axs[1].set_title(\"After normalization\")\n",
    "\n",
    "# make scatterplot to verify that between-species differences remain after you've transformed the data\n",
    "sns.scatterplot(data=X_train, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=Y, palette = pal[0:2], alpha=0.9, s=50, ax=axs[2])\n",
    "axs[2].set_title(\"scatterplot of normalized data\")\n",
    "\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, even though we've transformed our features so that they both have mean 0 and standard deviation 1 (left and middle plot above), when we look at the values for each species, the differences between them that we'll use for classification are still preserved (right plot).\n",
    "\n",
    "### Data leakage\n",
    "\n",
    "Note how I'm rescaling the training data and test data ***separately***. If you want to treat the test data as genuinely new -- the model has never seen anything about it --  then you should normalize your training features separately from the test data (with $\\mu_{training}$ and $\\sigma_{training}$). While it may sometimes not make a difference, if you instead e.g. calculate $\\sigma$ using the entire dataset when normalizing features, you've inadvertently learned something about the test data through using it to compute the variance! The end result is a model that is deceptively accurate and ends up performing worse anytime anyone else uses it on their new data, leading to a [reproducibility crisis](https://www.sciencedirect.com/science/article/pii/S2666389923001599#fig3)\n",
    "\n",
    "Generally, data leakage can occurr anytime you use the entire dataset for data pre-propcessing, such as feature rescaling like we just showed but also *data imputation*!\n",
    "\n",
    "Another example of data leakage is if you have duplicates, and one copy of each ends up in training and test set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *simplest* ML algorithm for classification: Logistic regression\n",
    "\n",
    "Logistic regression uses a linear function (very interpretable) to model the log odds, or the likelihood of a particular outcome. Specifically, the odds is the ratio of the probability an event ocurrs to the probability it does not occurr. \n",
    "\n",
    "$$ log{P(Gentoo) \\over P(Adelie)} = \\beta_0 + \\beta_{1}x_1 + \\beta_{2}x_2$$\n",
    "\n",
    "Here, we will use scikit-learn's `LogisticRegression` classifier, which has two build-in methods we'll use:\n",
    "- .fit() to feed it our data\n",
    "- .score() to get the \"accuracy\" of the model, which is simply the fraction of correct classifications\n",
    "\n",
    "In the lecture I mentioned choosing *hyperparameter* values, training many models (one for each hyperparamter value) and selecting the best one. However, logistic regression doesn't have any critical hyperparameters. You could vary the stength of \"regularization\", which is a very important topic, but a bit advanced and also not necessary for these data. \n",
    "\n",
    "However, since it's important to know the basics: \"regularization\" penalizes models for using all the features we give it, forcing it to use only the most informative features. ***How much*** it penalizes the model is a number >= 0 and can be treated as a *hyperparameter*, i.e. select a bunch of values, train a model for each one, and select whatever model has best predictive performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize classifier, hyperparameters would go in the parentheses\n",
    "classifier = LogisticRegression()\n",
    "# fit the model using the training data\n",
    "model_fit = classifier.fit(X_train, Y_train)\n",
    "\n",
    "# get model accuracy on training data\n",
    "train_accuracy = model_fit.score(X_train, Y_train)\n",
    "\n",
    "# get model accuracy on test data\n",
    "test_accuracy = model_fit.score(X_test, Y_test)\n",
    "\n",
    "# print results!\n",
    "print(\"Training accuracy: \", train_accuracy)\n",
    "print(\"Test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW! That's an accurate model! How is 100% even possible? Because this is easy mode. \n",
    "\n",
    "Let's see how it made the decision boundary.\n",
    "\n",
    "The following code is a little complicated. The only important thing here is to see how the model is making decisions. To avoid having a bunch of redundant code, I'll make a couple of functions to get the decision boundary and then plot the boundary with the training data and the test data, separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cmap = ListedColormap(sorted(penguins['color'].unique())) # convert our color palette to a matplotlib colormap object\n",
    "\n",
    "# plotting functions to avoid repeating code\n",
    "\n",
    "# plot decision boudnary\n",
    "def get_decision_boundary(X, classifier, fig_panel):\n",
    "    # this function returns a decision boundary display object, and we can specify the figure panel to plot it on\n",
    "    return DecisionBoundaryDisplay.from_estimator(\n",
    "        classifier, X, response_method=\"predict\",\n",
    "        xlabel=X.columns[0], ylabel=X.columns[1],\n",
    "        alpha=0.5, cmap=cmap, ax=axs[fig_panel])\n",
    "\n",
    "# add data points to the plot with decision boundary\n",
    "def plot_decision_boundary_with_data(disp, X, Y):\n",
    "    # this function takes a decision boundary display object, and plots it along with the data\n",
    "    disp.ax_.scatter(X[X.columns[0]], X[X.columns[1]], c=penguins['color'].loc[Y.index], edgecolor=\"k\")\n",
    "    sns.despine()\n",
    "\n",
    "# design the plot\n",
    "\n",
    "# specify the layout\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "\n",
    "# get the decision boundary objects to display\n",
    "disp_train = get_decision_boundary(X_train, model_fit, fig_panel=0)\n",
    "disp_test = get_decision_boundary(X_train, model_fit, fig_panel=1)\n",
    "\n",
    "# plot decision boundary along with training and test data\n",
    "plot_decision_boundary_with_data(disp_train, X_train, Y_train)\n",
    "plot_decision_boundary_with_data(disp_test, X_test, Y_test)\n",
    "\n",
    "# make titles\n",
    "axs[0].set_title(\"Training Data\")\n",
    "axs[1].set_title(\"Test Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model made a decision boundary right between the data that are highly clustered by species.\n",
    "\n",
    "Things get interesting on hard mode, where it's nice to have the model make a decision for us because it's not immediately obvious.\n",
    "\n",
    "\n",
    "\n",
    "Lastly, how do each of the variables contribute to the prediction? What has our model learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the coefficients\n",
    "print(\"Features: \", list(X_train.columns))\n",
    "print(\"Coefficients: \", model_fit.coef_)\n",
    "print(\"Intercept: \", model_fit.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of these coefficients is the following:\n",
    "\n",
    "**Positive**: as the value of that feature ***increases***, the response (log odds) ***increases***. Bill length is positive.\n",
    "\n",
    "**Negative**: as the value of that feature ***increases***, the response (log odds) ***decreases***. Bill depth is negative.\n",
    "\n",
    "**Magnitude**: how strongly each feature drives the value of the response (the log odds), because we've normalized all our features to be on the same scale, we can directly compare them!\n",
    "\n",
    "We can also use this model to predict new hypothetical samples! Look at the decision boundary above, and select a pair of feature values, and see what the predicted species is to see if everything's working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data has to be a 2D array, so instead of giving the .predict() function a list [] of features, we have to use a nested list [[]]\n",
    "print( model_fit.predict([[2,-1]]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree classifier\n",
    "\n",
    "Our accuracy is already incredibly good with the simplest ML classifier, which isn't too surprising given these data are also very simple. If the simplest model already has > 99% accuracy, there's little point in using more complex models.\n",
    "\n",
    "However, to illustrate another extremely popular ML model that's more complex than logistic regression, let's make a decision tree to classify these penguin species.\n",
    "\n",
    "Decision trees use a series of \"if-then\" rules to create a decision boundary, splitting feature space into regions that are as homogenous as possible in order to separate the species we're trying to classify. These 'if-then' rules can create more complicated decision boundaries than simple straight lines, which are made by logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train a decision tree classifier using our training data.\n",
    "\n",
    "In lecture, I mentioned that decision trees have a hyperparameter that we specify beforehand: the number of splits we allow it to make. \n",
    "\n",
    "Let's allow the decision tree to make anywhere from 1 split (the fewest) to 30 splits (probably way too much, given simplicity of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify max number of splits\n",
    "max_splits = 30\n",
    "# make a list of all the number of splits we want to test, using the built-in range() python function\n",
    "splits_to_make = range(1, max_splits + 1)\n",
    "\n",
    "print(list(splits_to_make))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our hyperparameter values stored in a list, let's iterate through them, training a decision tree each time.\n",
    "\n",
    "We do this using a for loop, where we have some 'dummy' variable called `s` that takes on a different value stored in the `splits_to_make` list until we've gone through the entire list. You can experiment with for loops to see how they work if you don't already!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty lists that will store scores for training and test data\n",
    "scores_train = []\n",
    "scores_test = []\n",
    "\n",
    "# train a model for each number of splits, record the accuracy\n",
    "for s in splits_to_make:\n",
    "    #initialize tree classifier with hyperparameter max_depth = d\n",
    "    T = DecisionTreeClassifier(max_depth = s, random_state = 11)\n",
    "    # fit model with training data using scikit learn fit() function\n",
    "    T.fit(X_train, Y_train)\n",
    "    # get accuracy using scikit learn score() function\n",
    "    train_accuracy = T.score(X_train, Y_train)\n",
    "    test_accuracy = T.score(X_test, Y_test)\n",
    "    # record the scores, the append() function adds the score to the end of the list\n",
    "    scores_train.append( train_accuracy )\n",
    "    scores_test.append( test_accuracy )\n",
    "\n",
    "# convert to numpy array to enable vectorized math operations\n",
    "scores_train = np.array(scores_train)\n",
    "scores_test = np.array(scores_test)\n",
    "\n",
    "# plot the results!!\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "# plot the accuracy\n",
    "sns.lineplot(x=splits_to_make, y = scores_train, color=\"blue\", ax=axs[0])\n",
    "sns.lineplot(x=splits_to_make, y = scores_test, color=\"red\", ax=axs[0])\n",
    "axs[0].set_xlabel(\"Max Depth hyperparameter\")\n",
    "axs[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "# plot the error, or 1 - accuracy\n",
    "sns.lineplot(x=splits_to_make, y = 1 - scores_train, color=\"blue\", ax=axs[1])\n",
    "sns.lineplot(x=splits_to_make, y = 1 - scores_test, color=\"red\", ax=axs[1])\n",
    "axs[1].set_xlabel(\"Max Depth hyperparameter\")\n",
    "axs[1].set_ylabel(\"Error (1 - accuracy)\")\n",
    "\n",
    "# add a legend showing the color of each line\n",
    "axs[0].legend( labels=[\"Training\", \"Test\"], labelcolor=[\"blue\", \"red\"], handlelength=0)\n",
    "axs[1].legend( labels=[\"Training\", \"Test\"], labelcolor=[\"blue\", \"red\"], handlelength=0)\n",
    "\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results look uninteresting because the data are simple, but when enabling hard mode they look much more interesting and familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "\n",
    "#Loop through depths from 1-30\n",
    "for d in splits_to_make:\n",
    "    #Decision Tree model with max_depth = d\n",
    "    # we use a 'pipeline' from sci_kit learn to combine the scaler and the classifier model, \n",
    "    # so that it renormalizes our data each time it splits the data using the scaler we gave it\n",
    "    model = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth = d, random_state = 11))\n",
    "    \n",
    "    # generate indices to split data into train/test sets\n",
    "    # note shuffle=True, this is important if your data table is structured by the response variable!\n",
    "    strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=11)\n",
    "\n",
    "    # calculate cross validation scores for each model, \n",
    "    # note that we give it the entire dataset, not just the training data, and we also are calculating the mean in the same line\n",
    "    cv_score_mean = cross_val_score(model, X, Y, cv = strat_k_fold).mean()\n",
    "\n",
    "    # store cross validation score\n",
    "    cv_scores.append( cv_score_mean )\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "sns.lineplot(x=splits_to_make, y = cv_scores, ax=axs[0])\n",
    "# plt.grid(axis='x', color='gray', linestyle='--', linewidth=1)\n",
    "axs[0].set_xlabel(\"Max Depth\")\n",
    "axs[0].set_ylabel(\"Cross Validation Accuracy\")\n",
    "\n",
    "sns.lineplot(x=splits_to_make, y = 1 - np.array(cv_scores), ax=axs[1])\n",
    "axs[1].set_xlabel(\"Max Depth\")\n",
    "axs[1].set_ylabel(\"Cross Validation Error (1-Accuracy)\")\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree model with max_depth = best_depth chosen from cross validation\n",
    "best_max_depth = 5\n",
    "T = DecisionTreeClassifier(max_depth = best_max_depth, random_state = 11)\n",
    "T.fit(X_train, Y_train)\n",
    "print(T.score(X_train, Y_train))\n",
    "print(T.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, we can plot the decision boundary for the decision tree. You can change the `best_max_depth` value above from 1 to 3 to see how it changes for different hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "\n",
    "# use the same function above, which we've made flexible enough to work with any classifier\n",
    "# this avoids making redundant code, which leads to very long notebooks that are hard to read!\n",
    "disp_train = get_decision_boundary(X_train, T, fig_panel=0)\n",
    "disp_test = get_decision_boundary(X_train, T, fig_panel=1)\n",
    "\n",
    "# plot decision boundary along with training and test data\n",
    "plot_decision_boundary_with_data(disp_train, X_train, Y_train)\n",
    "plot_decision_boundary_with_data(disp_test, X_test, Y_test)\n",
    "\n",
    "# make titles\n",
    "axs[0].set_title(\"Training Data\")\n",
    "axs[1].set_title(\"Test Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the many nice things about decision trees is the ability to calculate ***feature importance scores***. Let compute these to see how the decision tree classifier is using each of the two features: bill length and bill depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a barplot of feature importance\n",
    "sns.barplot(x=list(X_train.columns), y=T.feature_importances_, color=\"gray\")\n",
    "plt.ylabel(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, on easy mode, the decision tree classifier deems bill depth as much more important in distinguishing between species. Does this make sense? If we look at the splits the decision tree made in the plots above, do splits on the bill depth axis seem to be dividing the data into more homogenous groups?\n",
    "\n",
    "If it thinks bill depth is so important, if we force the tree to only get to make 1 split (set `best_max_depth`=1), does it choose the bill depth feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make visualize exactly how the decision tree splits the data and makes a final decision as to what species to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "artists = plot_tree(T, feature_names=X_train.columns, class_names=penguins['species'].unique(), filled=True, fontsize=8)\n",
    "\n",
    "for artist, impurity, value in zip(artists, T.tree_.impurity, T.tree_.value):\n",
    "    # let the max value decide the color; whiten the color depending on impurity (gini)\n",
    "    r, g, b = to_rgb(pal[np.argmax(value)])\n",
    "    f = impurity * (3/2) # for N colors: f = impurity * N/(N-1) if N>1 else 0\n",
    "    artist.get_bbox_patch().set_facecolor((f + (1-f)*r, f + (1-f)*g, f + (1-f)*b))\n",
    "    artist.get_bbox_patch().set_edgecolor('black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this model we've just trained to predict new data using the `predict()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( T.predict([[-1, 0]]) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
